{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from os import listdir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_brain_contour(image, plot=False):\n",
    "    \n",
    "    #import imutils\n",
    "    #import cv2\n",
    "    #from matplotlib import pyplot as plt\n",
    "    \n",
    "    # Convert the image to grayscale, and blur it slightly\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image, then perform a series of erosions +\n",
    "    # dilations to remove any small regions of noise\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # Find contours in thresholded image, then grab the largest one\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    \n",
    "\n",
    "    # Find the extreme points\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    \n",
    "    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n",
    "    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]            \n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        plt.tick_params(axis='both', which='both', \n",
    "                        top=False, bottom=False, left=False, right=False,\n",
    "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "        \n",
    "        plt.title('Original Image')\n",
    "            \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(new_image)\n",
    "\n",
    "        plt.tick_params(axis='both', which='both', \n",
    "                        top=False, bottom=False, left=False, right=False,\n",
    "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "\n",
    "        plt.title('Cropped Image')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_list, image_size):\n",
    "    \"\"\"\n",
    "    Read images, resize and normalize them. \n",
    "    Arguments:\n",
    "        dir_list: list of strings representing file directories.\n",
    "    Returns:\n",
    "        X: A numpy array with shape = (#_examples, image_width, image_height, #_channels)\n",
    "        y: A numpy array with shape = (#_examples, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # load all images in a directory\n",
    "    X = []\n",
    "    y = []\n",
    "    image_width, image_height = image_size\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        for filename in listdir(directory):\n",
    "            # load the image\n",
    "            image = cv2.imread(directory + '\\\\' + filename)\n",
    "            # crop the brain and ignore the unnecessary rest part of the image\n",
    "            image = crop_brain_contour(image, plot=False)\n",
    "            # resize image\n",
    "            image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
    "            # normalize values\n",
    "            image = image / 255.\n",
    "            # convert image to numpy array and append it to X\n",
    "            X.append(image)\n",
    "            # append a value of 1 to the target array if the image\n",
    "            # is in the folder named 'yes', otherwise append 0.\n",
    "            if directory[-3:] == 'yes':\n",
    "                y.append([1])\n",
    "            else:\n",
    "                y.append([0])\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    print(f'Number of examples is: {len(X)}')\n",
    "    print(f'X shape is: {X.shape}')\n",
    "    print(f'y shape is: {y.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 2065\n",
      "X shape is: (2065, 240, 240, 3)\n",
      "y shape is: (2065, 1)\n"
     ]
    }
   ],
   "source": [
    "augmented_path = 'augmented data/'\n",
    "\n",
    "# augmented data (yes and no) contains both the original and the new generated examples\n",
    "augmented_yes = augmented_path + 'yes' \n",
    "augmented_no = augmented_path + 'no'\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = (240, 240)\n",
    "\n",
    "X, y = load_data([augmented_yes, augmented_no], (IMG_WIDTH, IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(X, y, n=50):\n",
    "    \"\"\"\n",
    "    Plots n sample images for both values of y (labels).\n",
    "    Arguments:\n",
    "        X: A numpy array with shape = (#_examples, image_width, image_height, #_channels)\n",
    "        y: A numpy array with shape = (#_examples, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    for label in [0,1]:\n",
    "        # grab the first n images with the corresponding y values equal to label\n",
    "        images = X[np.argwhere(y == label)]\n",
    "        n_images = images[:n]\n",
    "        \n",
    "        columns_n = 10\n",
    "        rows_n = int(n/ columns_n)\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        i = 1 # current plot        \n",
    "        for image in n_images:\n",
    "            plt.subplot(rows_n, columns_n, i)\n",
    "            plt.imshow(image[0])\n",
    "            \n",
    "            # remove ticks\n",
    "            plt.tick_params(axis='both', which='both', \n",
    "                            top=False, bottom=False, left=False, right=False,\n",
    "                           labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        label_to_str = lambda label: \"Yes\" if label == 1 else \"No\"\n",
    "        plt.suptitle(f\"Brain Tumor: {label_to_str(label)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "       \n",
    "    \"\"\"\n",
    "    Splits data into training, development and test sets.\n",
    "    Arguments:\n",
    "        X: A numpy array with shape = (#_examples, image_width, image_height, #_channels)\n",
    "        y: A numpy array with shape = (#_examples, 1)\n",
    "    Returns:\n",
    "        X_train: A numpy array with shape = (#_train_examples, image_width, image_height, #_channels)\n",
    "        y_train: A numpy array with shape = (#_train_examples, 1)\n",
    "        X_val: A numpy array with shape = (#_val_examples, image_width, image_height, #_channels)\n",
    "        y_val: A numpy array with shape = (#_val_examples, 1)\n",
    "        X_test: A numpy array with shape = (#_test_examples, image_width, image_height, #_channels)\n",
    "        y_test: A numpy array with shape = (#_test_examples, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1445\n",
      "number of development examples = 310\n",
      "number of test examples = 310\n",
      "X_train shape: (1445, 240, 240, 3)\n",
      "Y_train shape: (1445, 1)\n",
      "X_val (dev) shape: (310, 240, 240, 3)\n",
      "Y_val (dev) shape: (310, 1)\n",
      "X_test shape: (310, 240, 240, 3)\n",
      "Y_test shape: (310, 1)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of development examples = \" + str(X_val.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_val (dev) shape: \" + str(X_val.shape))\n",
    "print (\"Y_val (dev) shape: \" + str(y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m}:{round(s,1)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(y_true, prob):\n",
    "    # convert the vector of probabilities to a target vector\n",
    "    y_pred = np.where(prob > 0.5, 1, 0)\n",
    "    \n",
    "    score = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Arugments:\n",
    "        input_shape: A tuple representing the shape of the input of the model. shape=(image_width, image_height, #_channels)\n",
    "    Returns:\n",
    "        model: A Model object.\n",
    "    \"\"\"\n",
    "    # Define the input placeholder as a tensor with shape input_shape. \n",
    "    X_input = Input(input_shape) # shape=(?, 240, 240, 3)\n",
    "    \n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((2, 2))(X_input) # shape=(?, 244, 244, 3)\n",
    "    \n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X) # shape=(?, 238, 238, 32)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((4, 4), name='max_pool0')(X) # shape=(?, 59, 59, 32) \n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((4, 4), name='max_pool1')(X) # shape=(?, 14, 14, 32)\n",
    "    \n",
    "    # FLATTEN X \n",
    "    X = Flatten()(X) # shape=(?, 6272)\n",
    "    # FULLYCONNECTED\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X) # shape=(?, 1)\n",
    "    \n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='BrainDetectionModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the image shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 240, 240, 3)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 244, 244, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 238, 238, 32)      4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 238, 238, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 238, 238, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 59, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 11,137\n",
      "Trainable params: 11,073\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "log_file_name = f'brain_tumor_detection_cnn_{int(time.time())}'\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{log_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "# unique file name that will include the epoch and the validation (development) accuracy\n",
    "filepath=\"cnn-parameters-improvement-{epoch:02d}-{val_acc:.2f}\"\n",
    "# save the model with the best validation (development) accuracy till now\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/10\n",
      "1445/1445 [==============================] - 434s 300ms/step - loss: 0.8331 - acc: 0.5945 - val_loss: 0.6829 - val_acc: 0.4968\n",
      "Epoch 2/10\n",
      "1445/1445 [==============================] - 463s 320ms/step - loss: 0.4817 - acc: 0.7668 - val_loss: 0.6342 - val_acc: 0.6742\n",
      "Epoch 3/10\n",
      "1445/1445 [==============================] - 471s 326ms/step - loss: 0.4361 - acc: 0.8069 - val_loss: 0.5294 - val_acc: 0.8065\n",
      "Epoch 4/10\n",
      "1445/1445 [==============================] - 465s 322ms/step - loss: 0.3641 - acc: 0.8574 - val_loss: 0.6092 - val_acc: 0.6323\n",
      "Epoch 5/10\n",
      "1445/1445 [==============================] - 457s 316ms/step - loss: 0.3940 - acc: 0.8339 - val_loss: 0.4689 - val_acc: 0.7742\n",
      "Epoch 6/10\n",
      "1445/1445 [==============================] - 452s 313ms/step - loss: 0.3154 - acc: 0.8692 - val_loss: 0.4448 - val_acc: 0.7806\n",
      "Epoch 7/10\n",
      "1445/1445 [==============================] - 465s 322ms/step - loss: 0.2776 - acc: 0.8872 - val_loss: 0.4747 - val_acc: 0.7323\n",
      "Epoch 8/10\n",
      "1445/1445 [==============================] - 439s 304ms/step - loss: 0.3271 - acc: 0.8519 - val_loss: 0.3655 - val_acc: 0.8516\n",
      "Epoch 9/10\n",
      "1445/1445 [==============================] - 435s 301ms/step - loss: 0.2182 - acc: 0.9190 - val_loss: 0.4557 - val_acc: 0.8129\n",
      "Epoch 10/10\n",
      "1445/1445 [==============================] - 438s 303ms/step - loss: 0.2054 - acc: 0.9225 - val_loss: 0.4038 - val_acc: 0.8129\n",
      "Elapsed time: 1:15:23.8\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train for a few more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/3\n",
      "1445/1445 [==============================] - 431s 299ms/step - loss: 0.2065 - acc: 0.9239 - val_loss: 0.3357 - val_acc: 0.8871\n",
      "Epoch 2/3\n",
      "1445/1445 [==============================] - 432s 299ms/step - loss: 0.1811 - acc: 0.9363 - val_loss: 0.3529 - val_acc: 0.8516\n",
      "Epoch 3/3\n",
      "1445/1445 [==============================] - 425s 294ms/step - loss: 0.1827 - acc: 0.9287 - val_loss: 0.4038 - val_acc: 0.8323\n",
      "Elapsed time: 0:21:29.4\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=3, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/3\n",
      "1445/1445 [==============================] - 438s 303ms/step - loss: 0.1471 - acc: 0.9612 - val_loss: 0.3190 - val_acc: 0.8903\n",
      "Epoch 2/3\n",
      "1445/1445 [==============================] - 432s 299ms/step - loss: 0.1384 - acc: 0.9564 - val_loss: 0.3509 - val_acc: 0.8613\n",
      "Epoch 3/3\n",
      "1445/1445 [==============================] - 429s 297ms/step - loss: 0.1240 - acc: 0.9647 - val_loss: 0.3358 - val_acc: 0.8710\n",
      "Elapsed time: 0:21:38.5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=3, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/3\n",
      "1445/1445 [==============================] - 536s 371ms/step - loss: 0.1586 - acc: 0.9453 - val_loss: 0.4005 - val_acc: 0.8548\n",
      "Epoch 2/3\n",
      "1445/1445 [==============================] - 427s 296ms/step - loss: 0.1244 - acc: 0.9647 - val_loss: 0.3149 - val_acc: 0.9000\n",
      "Epoch 3/3\n",
      "1445/1445 [==============================] - 429s 297ms/step - loss: 0.1074 - acc: 0.9668 - val_loss: 0.3118 - val_acc: 0.8935\n",
      "Elapsed time: 0:23:11.9\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=3, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1445 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "1445/1445 [==============================] - 427s 296ms/step - loss: 0.0899 - acc: 0.9785 - val_loss: 0.3310 - val_acc: 0.8935\n",
      "Epoch 2/5\n",
      "1445/1445 [==============================] - 426s 295ms/step - loss: 0.1343 - acc: 0.9509 - val_loss: 0.5169 - val_acc: 0.8258\n",
      "Epoch 3/5\n",
      "1445/1445 [==============================] - 425s 294ms/step - loss: 0.1137 - acc: 0.9626 - val_loss: 0.6945 - val_acc: 0.7516\n",
      "Epoch 4/5\n",
      "1445/1445 [==============================] - 430s 298ms/step - loss: 0.1018 - acc: 0.9640 - val_loss: 0.3210 - val_acc: 0.9065\n",
      "Epoch 5/5\n",
      "1445/1445 [==============================] - 434s 300ms/step - loss: 0.0949 - acc: 0.9689 - val_loss: 0.4250 - val_acc: 0.8484\n",
      "Elapsed time: 0:35:41.9\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)\n",
    "print(f\"Elapsed time: {hms_string(execution_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d4bcd7f14c1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss\n",
      "val_acc\n",
      "loss\n",
      "acc\n"
     ]
    }
   ],
   "source": [
    "for key in history.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    \n",
    "    train_loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    train_acc = history['acc']\n",
    "    val_acc = history['val_acc']\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-713e6b6b9a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_metrics(history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\thinkpad\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\thinkpad\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(filepath='models/cnn-parameters-improvement-23-0.91.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 32s 103ms/sample - loss: 0.1545 - acc: 0.9516\n"
     ]
    }
   ],
   "source": [
    "loss, acc = best_model.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.15446051500978009\n",
      "Test Accuracy = 0.9516128897666931\n"
     ]
    }
   ],
   "source": [
    "print (f\"Test Loss = {loss}\")\n",
    "print (f\"Test Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9501661129568105\n"
     ]
    }
   ],
   "source": [
    "f1score = compute_f1_score(y_test, y_test_prob)\n",
    "print(f\"F1 score: {f1score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9426751592356688\n"
     ]
    }
   ],
   "source": [
    "f1score_val = compute_f1_score(y_val, y_val_prob)\n",
    "print(f\"F1 score: {f1score_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, y_val_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['no','yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b0c8b9e61200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\thinkpad\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thinkpad\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "#cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)\n",
    "cm = confusion_matrix(y_test, y_test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-c55cb2e7f60b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalues_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_measure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-c62d2e9dae18>\u001b[0m in \u001b[0;36mperf_measure\u001b[1;34m(y_actual, y_pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0my_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m            \u001b[0mTP\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "values_imp = perf_measure(y_test, X_test)\n",
    "print(values_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = y_test_prob #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "y_test_prob_n = pd.DataFrame(x_scaled)\n",
    "#y_test_prob_n\n",
    "y_test_prob_b = preprocessing.binarize(y_test_prob_n,0.5)\n",
    "y_test_prob_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 7, 152, 8)\n"
     ]
    }
   ],
   "source": [
    "values_imp = perf_measure(y_test, y_test_prob_b)\n",
    "print(values_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
